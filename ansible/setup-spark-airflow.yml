---
- name: Configure nodes
  hosts: spark_nodes
  tasks:
    - include_role:
        name: cis-benchmark
    - name: Mount /dev/sdb into /opt/app
      become: true
      mount:
        path: /opt/app
        src: /dev/sdb
        fstype: ext4
        state: present
    - include_role:
        name: install-aws-cli
    - include_role:
        name: install-guardduty-agent
    - include_role:
        name: install-docker
    - name: Install ECR credentials helper
      become: true
      vars:
        version: 0.9.1
        checksums:
          arm64: sha256:a10012acfe5e28d7aed18f06bec4aa2a13fb3d9765898c36ef31136b24bd56e9
          amd64: sha256:c0054f2635b2f01b00f7bf6f88023ffe7fded15c533a85a493037607135eebac
      get_url:
        url: https://amazon-ecr-credential-helper-releases.s3.us-east-2.amazonaws.com/{{ version }}/linux-{{ 'arm64' if ansible_facts.architecture == 'aarch64' else 'amd64' }}/docker-credential-ecr-login
        dest: /usr/bin/docker-credential-ecr-login
        mode: "0755"
        checksum: "{{ checksums['arm64' if ansible_facts.architecture == 'aarch64' else 'amd64'] }}"
    - name: Create directory for Docker configuration
      become: true
      file:
        path: /root/.docker
        state: directory
    - name: Setup Docker configuration
      become: true
      copy:
        content: |
          {
            "credsStore": "ecr-login"
          }
        dest: /root/.docker/config.json
    - name: Install pip
      become: true
      dnf:
        name: python3-pip
        state: present
    - name: Install Python dependencies
      become: true
      pip:
        name:
          - docker
          - jsondiff
        state: present
    - name: Pull Spark image
      become: true
      docker_image:
        name: "{{ spark_image }}"
        source: pull
- import_playbook: playbooks/setup-swarm-cluster.yaml
- name: Setup Spark and Airflow
  hosts: spark_main_node
  vars:
    working_directory: /opt/mobicom
  tasks:
    - name: Create overlay network
      become: true
      docker_network:
        name: mobi-network
        driver: overlay
        attachable: true
    - name: Get primary node ID
      become: true
      # See: https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_advanced_syntax.html#unsafe-or-raw-strings
      command: !unsafe docker node ls -f role=manager --format {{.ID}}
      register: primary_node_id
    - name: Set primary node Spark role
      become: true
      docker_node:
        hostname: "{{ primary_node_id.stdout }}"
        labels:
          spark-role: master
    - name: Get worker node IDs
      become: true
      command: !unsafe docker node ls -f role=worker --format {{.ID}}
      register: worker_node_ids
    - name: Set worker node Spark role
      become: true
      docker_node:
        hostname: "{{ item }}"
        labels:
          spark-role: worker
      with_items: "{{ worker_node_ids.stdout_lines }}"
    - name: Check if lakehouse repository is cloned
      stat:
        path: "{{ working_directory }}"
      register: lakehouse_repo
      failed_when: false
    - name: Clone lakehouse repository
      become: true
      block:
        - name: Prepare directory for project
          file:
            path: /opt
            state: directory
        - name: Copy deployment key
          copy:
            # See: https://github.com/ansible/ansible/issues/30829#issuecomment-334192427
            content: "{{ lookup('file', 'files/deploy-key') + '\n' }}"
            dest: /tmp/lakehouse-deployment-key
            mode: 0600
        - name: Ensure Git is installed
          package:
            name: git
            state: present
        - name: Clone repository
          git:
            repo: git@github.com:erdenebayrd/mobicom.git
            key_file: /tmp/lakehouse-deployment-key
            accept_hostkey: true
            single_branch: true
            dest: "{{ working_directory }}"
        - name: Remove deployment key
          shell: shred -u /tmp/lakehouse-deployment-key
        - name: Ensure owner of repository files
          file:
            path: "{{ working_directory }}"
            owner: "{{ ansible_user }}"
            group: "{{ ansible_user }}"
            mode: "0755"
            recurse: true
      when: not lakehouse_repo.stat.exists
    - name: Start Spark stack
      become: true
      docker_stack:
        state: present
        name: spark
        compose:
          - "{{ working_directory }}/.docker/spark/stack.yaml"
    - name: Configure dotenv file for Airflow
      vars:
        airflow_uid: "{{ ansible_user_uid }}"
      template:
        src: files/airflow.env.j2
        dest: "{{ working_directory }}/.env"
    - name: Start Airflow
      become: true
      community.docker.docker_compose_v2:
        project_src: "{{ working_directory }}/.docker/airflow"
      environment:
        AIRFLOW_UID: "{{ ansible_user_uid }}"
